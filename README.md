# Search For Efficient Defense Mechanism Against $l_0$-norm Bound Attacks

Eric Yoon, University of California - Santa Barbara, changhee@ucsb.edu

# Abstract

Adversarial attacks pose significant challenges to the robustness and reliability of deep learning models. This project investigates a defense mechanism against l0
norm bound adversarial attacks in the context of text classifiers, inspired by the truncation method used in computer vision and guaranteed robustness from input augmentation with random masking. The primary goal of this project was to develop a method for efficiently detecting perturbed words in input sentences with high accuracy, consequently masking them to enhance the robustness of text classifier models. We explored various approaches to measure word importance, including frequency-based importance scales such as TF-IDF and vulnerability-based measures. Despite our efforts, the proposed methods did not yield satisfactory results in defending against adversarial attacks. Though implementing the attack mechanism to mask words did make the model more computationally intensive to generate successful attacks. This paper presents an overview of the conducted research, the methodologies employed, and a discussion on potential improvements for future investigations.

Continued in the report.html
